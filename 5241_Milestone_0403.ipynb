{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaiaX926/P-MNIST-41milestone/blob/main/5241_Milestone_0403.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsllEUh6f2eQ"
      },
      "source": [
        "# 1. Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SyLxh4eAtu0V"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6M0ySzktwp_",
        "outputId": "8a4686bd-4fb9-4470-cce7-6d184be2bd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ],
      "source": [
        "DOWNLOAD_MNIST = True # If already download , set as False\n",
        "train_data = torchvision . datasets . MNIST (\n",
        "  root ='./ mnist /',\n",
        "  train = True , # this is training data\n",
        "  # transform = torchvision . transforms . ToTensor () ,\n",
        "  download = DOWNLOAD_MNIST ,\n",
        ")\n",
        "test_data = torchvision . datasets . MNIST ( root ='./ mnist /', train = False )\n",
        "\n",
        "# change the features to numpy\n",
        "X_train = train_data . train_data . numpy ()\n",
        "X_test = test_data . test_data . numpy ()\n",
        "\n",
        "# change the labels to numpy\n",
        "Y_train = train_data . train_labels . numpy ()\n",
        "Y_test = test_data . test_labels . numpy ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_LYOAJuuO3S"
      },
      "source": [
        "# 2.Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z47_RU8LuUkH"
      },
      "source": [
        "## **(a)** \n",
        "Plot one sample in X train. What is the number you see from the 28 Ã— 28 pixel-field? Does it match with the label in Y train?\n",
        "\n",
        "A: Yes. The image of figure 3 has the label as 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cKY5nZISuEMy",
        "outputId": "264718f4-b829-4569-ef69-8805cd263e5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "image_idx = np.random.choice(range(X_train.shape[0]))\n",
        "image = X_train[image_idx]\n",
        "image_class = Y_train[image_idx]\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(X_train[image_idx].astype(\"uint8\"))\n",
        "plt.title(image_class)\n",
        "_ = plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWzB2QFwAUM"
      },
      "source": [
        "## **(b)** \n",
        "What is the dimension of X train and X test? Normalize X train and X test such that the value of each element lies in [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpy7fQguwHHs",
        "outputId": "c57ffc76-32e8-4ee4-9afb-580fe2932fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Ku8zCizDl8",
        "outputId": "42236fa9-834a-47bc-e575-4b5b80bf9aca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(255, 0, 255, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "np.max(X_train),np.min(X_train),np.max(X_test),np.min(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9rqowL0wWH5",
        "outputId": "a9df5d0d-71ab-4a7f-8779-380080255832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n",
            "1.0 0.0 1.0 0.0\n"
          ]
        }
      ],
      "source": [
        "X_train_normalized = X_train/255\n",
        "X_test_normalized = X_test/255\n",
        "\n",
        "# for i in range(len(X_train)):\n",
        "#   X_train_normalized[i] = (X_train[i] - np.mean(X_train[i])) / np.std(X_train[i])\n",
        "\n",
        "# for i in range(len(X_test)):\n",
        "#   X_test_normalized[i] = (X_test[i] - np.mean(X_test[i])) / np.std(X_test[i])\n",
        "\n",
        "print(X_train_normalized.shape, X_test_normalized.shape)\n",
        "print(np.max(X_train_normalized), np.min(X_train_normalized),np.max(X_test_normalized), np.min(X_test_normalized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOUPaegT5QAy"
      },
      "source": [
        "## **(c)** \n",
        "A popular choice to deal with the labels is to use the one-hot embedding. Represent\n",
        "Y train and Y test using one-hot embedding. List the benefit of such transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vwqZ0uV2wyd",
        "outputId": "c79b1644-a098-4b1b-a713-2a8b1bbabc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10) [0. 1.]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(Y_train, num_classes=10)\n",
        "test_labels = to_categorical(Y_test, num_classes=10)\n",
        "print(train_labels.shape, np.unique(train_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh7tOCAy6KrM"
      },
      "source": [
        "1. It eliminates the influence of the numeric value labels in prediction. For example, the computer might assume group 1 is closer to group 2 than to group 9. However, the distance between them is not affected by the group number at all.\n",
        "2. If the tags are strings, one-hot embedding will help transfer the labels into numbers for predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zYGTIqUFVgh"
      },
      "source": [
        "# 3. Before Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjMJJRW_FQhi"
      },
      "source": [
        "## **(a)** \n",
        "Try to implement and train the above mentioned classifier on the training\n",
        "dataset, and report the test errors of them using the test dataset. Can you reproduce\n",
        "the results? If not, please justify your reason\n",
        "\n",
        "A: The specific test error cannot be reproduced since the random state is not anchored. The accuracies of KNN and SVM are similar to the output in the paper while the outcome of Adaboost is much lower than expected. I guess it's because there are so many parameters that need to be specified in Adaboost. Also, the decision tree is prone to overfit, which may make it a bad predictor in this condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eKodxeo-2ABF"
      },
      "outputs": [],
      "source": [
        "def errorrate(y_pred, y_true):\n",
        "  error = sum([y_pred[i]!=y_true[i] for i in range(len(y_true))])\n",
        "  return error/len(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXFLPtEFvsw"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Io69BuFLQ-VQ"
      },
      "outputs": [],
      "source": [
        "# x_train = np.reshape(X_train_normalized,(60000,28*28))\n",
        "# x_test = np.reshape(X_test_normalized,(10000,28*28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tj7pm-lFwy0",
        "outputId": "4809964f-1717-44f6-cdeb-d371203449c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error of KNN is:  0.0335\n",
            "Accuracy_score of KNN is:  0.9665\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "KNN = KNeighborsClassifier(n_neighbors=10)\n",
        "KNN.fit(np.reshape(X_train_normalized,(60000,28*28)),Y_train)\n",
        "knn_prediction = KNN.predict(np.reshape(X_test_normalized,(10000,28*28)))\n",
        "KNN_accuracy = accuracy_score(Y_test, knn_prediction)\n",
        "print('Test Error of KNN is: ', errorrate(knn_prediction,Y_test))\n",
        "print('Accuracy_score of KNN is: ', KNN_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiMjsjjRUeLR"
      },
      "source": [
        "### AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD0Z0nEZUede",
        "outputId": "8fd0299a-37c9-443f-8d8a-f29dd9ea288d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error of AdaBoost is:  0.1825\n",
            "Accuracy_score of AdaBoost is:  0.8175\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ABC = AdaBoostClassifier(n_estimators=500, learning_rate=0.1)\n",
        "ABC.fit(np.reshape(X_train_normalized,(60000,28*28)),Y_train)\n",
        "abc_prediction = ABC.predict(np.reshape(X_test_normalized,(10000,28*28)))\n",
        "ABC_accuracy = accuracy_score(Y_test, abc_prediction)\n",
        "print('Test Error of AdaBoost is: ', errorrate(abc_prediction,Y_test))\n",
        "print('Accuracy_score of AdaBoost is: ', ABC_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGv7nVNUUSM"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIM9blYnSS-l",
        "outputId": "57659779-365f-4b7d-f3d7-d033626a762c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error of SVM is:  0.0208\n",
            "Accuracy_score of SVM is:  0.9792\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "SVM = SVC(kernel='rbf')\n",
        "SVM.fit(np.reshape(X_train_normalized,(60000,28*28)),Y_train)\n",
        "svm_prediction = SVM.predict(np.reshape(X_test_normalized,(10000,28*28)))\n",
        "SVM_accuracy = accuracy_score(Y_test, svm_prediction)\n",
        "print('Test Error of SVM is: ', errorrate(svm_prediction,Y_test))\n",
        "print('Accuracy_score of SVM is: ', SVM_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMWFZpvyaqxO"
      },
      "source": [
        "## **(b)**\n",
        "Pick your favorite classifier (not limited to the above mentioned algorithms)\n",
        "and try to implement it on the training set and report the test error using the test\n",
        "dataset. Turn the hyperparameters until it out perform all three of the classifier you\n",
        "implemented in part 2(a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAbxBlKHPfRz"
      },
      "source": [
        "### SVM GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xEIcPqf7Ts7b"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# #parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'), 'C':[1, 5]}\n",
        "# #parameters = {'C':[0.1, 0.5, 1, 2],'degree':[3,4,5]}\n",
        "# parameters = {'degree':[3,4,5,6,7]}\n",
        "\n",
        "# clf_svc = SVC(kernel = 'poly')\n",
        "# clf = GridSearchCV(clf_svc, parameters, refit=True)\n",
        "# clf.fit(np.reshape(X_train_normalized,(60000,28*28)),Y_train)\n",
        "# #print('The model we pick is SVM with:', clf.best_params_())\n",
        "# clf_prediction = clf.predict(np.reshape(X_test_normalized,(10000,28*28)))\n",
        "# clf_accuracy = accuracy_score(Y_test, clf_prediction)\n",
        "# print('Test Error of a better SVM is: ', errorrate(clf_prediction,Y_test))\n",
        "# print('Accuracy_score of a better SVM is: ', clf_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax2t5knsSd4Z"
      },
      "source": [
        "### Gaussian Navie Bayesian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3ZumG5DZSf8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c6566b-09d7-40ea-b8a9-f6b0d533d490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error of a better GaussianNB is:  0.4442\n",
            "Accuracy_score of a better GaussianNB is:  0.5558\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(np.reshape(X_train_normalized,(60000,28*28)),Y_train)\n",
        "gnb_prediction = gnb.predict(np.reshape(X_test_normalized,(10000,28*28)))\n",
        "gnb_accuracy = accuracy_score(Y_test, gnb_prediction)\n",
        "print('Test Error of a better GaussianNB is: ', errorrate(gnb_prediction,Y_test))\n",
        "print('Accuracy_score of a better GaussianNB is: ', gnb_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tv8jxiS-1xFq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "oYzhb96o7VBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ik0txJ2t5Ic3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#import helper\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# moves your model to train on your gpu if available else it uses your cpu\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transform to normalize data\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                ])\n",
        "\n",
        "\n",
        "\n",
        "# Download and load the training data\n",
        "train_set = datasets.MNIST('DATA_MNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "OVV3ECxx5Q79"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        # Convolutional Neural Network Layer \n",
        "        self.convolutaional_neural_network_layers = nn.Sequential(\n",
        "                # Here we are defining our 2D convolutional layers\n",
        "                # We can calculate the output size of each convolutional layer using the following formular\n",
        "                # outputOfEachConvLayer = [(in_channel + 2*padding - kernel_size) / stride] + 1\n",
        "                # We have in_channels=1 because our input is a grayscale image\n",
        "                nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, padding=1, stride=1), # (N, 1, 28, 28) \n",
        "                nn.ReLU(),\n",
        "                # After the first convolutional layer the output of this layer is:\n",
        "                # [(28 + 2*1 - 3)/1] + 1 = 28. \n",
        "                nn.MaxPool2d(kernel_size=2), \n",
        "                # Since we applied maxpooling with kernel_size=2 we have to divide by 2, so we get\n",
        "                # 28 / 2 = 14\n",
        "          \n",
        "                # output of our second conv layer\n",
        "                nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, padding=1, stride=1),\n",
        "                nn.ReLU(),\n",
        "                # After the second convolutional layer the output of this layer is:\n",
        "                # [(14 + 2*1 - 3)/1] + 1 = 14. \n",
        "                nn.MaxPool2d(kernel_size=2) \n",
        "                # Since we applied maxpooling with kernel_size=2 we have to divide by 2, so we get\n",
        "                # 14 / 2 = 7\n",
        "        )\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear_layers = nn.Sequential(\n",
        "                # We have the output_channel=24 of our second conv layer, and 7*7 is derived by the formular \n",
        "                # which is the output of each convolutional layer\n",
        "                nn.Linear(in_features=24*7*7, out_features=64),          \n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=0.2), # Dropout with probability of 0.2 to avoid overfitting\n",
        "                nn.Linear(in_features=64, out_features=10) # The output is 10 which should match the size of our class\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass \n",
        "    def forward(self, x):\n",
        "        x = self.convolutaional_neural_network_layers(x)\n",
        "        # After we get the output of our convolutional layer we must flatten it or rearrange the output into a vector\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Then pass it through the linear layer\n",
        "        x = self.linear_layers(x)\n",
        "        # The softmax function returns the prob likelihood of getting the input image. \n",
        "        # We will see a much graphical demonstration below\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TO94OaB85exM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network()\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYVoVJjj5im5",
        "outputId": "0d0d9f02-60e5-4f16-b1cd-e2df667019a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (convolutaional_neural_network_layers): Sequential(\n",
            "    (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=1176, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "D-15p_xs5itJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20 # The total number of iterations\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # prep model for training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for idx, (images, labels) in enumerate(trainloader): \n",
        "\n",
        "        # Send these >>> To GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        #Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    else:\n",
        "        # prep model for evaluation\n",
        "        model.eval() \n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "\n",
        "        # Turn off the gradients when performing validation.\n",
        "        # If we don't turn it off, we will comprise our networks weight entirely\n",
        "        with torch.no_grad():\n",
        "            for images, labels in testloader:\n",
        "                \n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                log_probabilities = model(images)\n",
        "                test_loss += criterion(log_probabilities, labels)\n",
        "\n",
        "                probabilities = torch.exp(log_probabilities)\n",
        "                top_prob, top_class = probabilities.topk(1, dim=1)\n",
        "                predictions = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(predictions.type(torch.FloatTensor))\n",
        "        \n",
        "        train_losses.append(train_loss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "        \n",
        "        print(\"Epoch: {}/{}  \".format(epoch+1, epochs),\n",
        "              \"Training loss: {:.4f}  \".format(train_loss/len(trainloader)),\n",
        "              \"Testing loss: {:.4f}  \".format(test_loss/len(testloader)),\n",
        "              \"Test accuracy: {:.4f}  \".format(accuracy/len(testloader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUo6hpzu5iwA",
        "outputId": "5ce91e84-9f00-44b2-a798-83fb88fd7e77"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20   Training loss: 0.7558   Testing loss: 0.2613   Test accuracy: 0.9251  \n",
            "Epoch: 2/20   Training loss: 0.2686   Testing loss: 0.1618   Test accuracy: 0.9499  \n",
            "Epoch: 3/20   Training loss: 0.1904   Testing loss: 0.1100   Test accuracy: 0.9658  \n",
            "Epoch: 4/20   Training loss: 0.1514   Testing loss: 0.0899   Test accuracy: 0.9719  \n",
            "Epoch: 5/20   Training loss: 0.1282   Testing loss: 0.0852   Test accuracy: 0.9724  \n",
            "Epoch: 6/20   Training loss: 0.1107   Testing loss: 0.0646   Test accuracy: 0.9790  \n",
            "Epoch: 7/20   Training loss: 0.0994   Testing loss: 0.0645   Test accuracy: 0.9779  \n",
            "Epoch: 8/20   Training loss: 0.0901   Testing loss: 0.0609   Test accuracy: 0.9797  \n",
            "Epoch: 9/20   Training loss: 0.0833   Testing loss: 0.0577   Test accuracy: 0.9804  \n",
            "Epoch: 10/20   Training loss: 0.0762   Testing loss: 0.0594   Test accuracy: 0.9795  \n",
            "Epoch: 11/20   Training loss: 0.0720   Testing loss: 0.0469   Test accuracy: 0.9856  \n",
            "Epoch: 12/20   Training loss: 0.0690   Testing loss: 0.0490   Test accuracy: 0.9834  \n",
            "Epoch: 13/20   Training loss: 0.0659   Testing loss: 0.0430   Test accuracy: 0.9849  \n",
            "Epoch: 14/20   Training loss: 0.0603   Testing loss: 0.0427   Test accuracy: 0.9856  \n",
            "Epoch: 15/20   Training loss: 0.0575   Testing loss: 0.0386   Test accuracy: 0.9876  \n",
            "Epoch: 16/20   Training loss: 0.0547   Testing loss: 0.0378   Test accuracy: 0.9874  \n",
            "Epoch: 17/20   Training loss: 0.0517   Testing loss: 0.0359   Test accuracy: 0.9881  \n",
            "Epoch: 18/20   Training loss: 0.0501   Testing loss: 0.0351   Test accuracy: 0.9879  \n",
            "Epoch: 19/20   Training loss: 0.0480   Testing loss: 0.0331   Test accuracy: 0.9886  \n",
            "Epoch: 20/20   Training loss: 0.0470   Testing loss: 0.0330   Test accuracy: 0.9889  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Final Accuracy_score of a CNN is:', accuracy/len(testloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zatk4yNOJtiA",
        "outputId": "9f9d7a2b-8f84-4b2b-cc64-03c1ed8aae16"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy_score of a CNN is: tensor(0.9889)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn_prediction = model.predict(X_test)\n",
        "# cnn_accuracy = accuracy_score(Y_test, cnn_prediction)\n",
        "# print('Test Error of a better CNN is: ', errorrate(cnn_prediction,Y_test))\n",
        "# print('Accuracy_score of a better CNN is: ', cnn_accuracy)"
      ],
      "metadata": {
        "id": "VxmnvN2M7ONf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BA0EzHA47OQE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BxKWdkkA1xIB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9Qoqk-etbrY7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6j3OrsNvbrbn"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YsllEUh6f2eQ",
        "Z47_RU8LuUkH",
        "1tWzB2QFwAUM",
        "SOUPaegT5QAy"
      ],
      "name": "5241-Milestone-0403.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmT4SOeN2wY69ld45ePmax",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}